{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf06865-c42e-4c89-a021-b9f0e6c6f075",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finger Extension Ability: How extended each finger is in real time. A fully extended finger shows near 100%, a curled finger shows lower values.\n",
    "\n",
    "Palm Size (Relative): How close or large the palm is relative to the camera frame size, useful to estimate hand distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c03a3aca-6027-4390-96bd-f467562e899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec\n",
    "\n",
    "custom_landmark_style = DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=6)  # Green landmarks, larger circles\n",
    "custom_connection_style = DrawingSpec(color=(0, 0, 255), thickness=3)                # Blue connections, thicker lines\n",
    "\n",
    "\n",
    "# Initialize the hands module\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def euclidean_dist(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "# Define the get_finger_extension function i\n",
    "def get_finger_extension(landmarks, tip_idx, base_idx):\n",
    "    # Calculate the extension as the ratio of current distance to maximum possible\n",
    "    current_dist = euclidean_dist(landmarks[tip_idx], landmarks[base_idx])\n",
    "    max_dist = euclidean_dist(landmarks[0], landmarks[9]) * 1.5  # Using palm size as reference\n",
    "    return min(current_dist / max_dist, 1.0)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "        \n",
    "    # Convert to RGB for MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "    \n",
    "    h, w, _ = frame.shape\n",
    "    \n",
    "    # Setup for visualization\n",
    "    info_x = 20\n",
    "    y_start = 50\n",
    "    gap = 60\n",
    "    bar_width = 200\n",
    "    bar_height = 20\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "            \n",
    "            # Convert normalized landmarks to pixel coordinates\n",
    "            landmarks = [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]\n",
    "            landmarks_px = [(int(x * w), int(y * h), z) for x, y, z in landmarks]\n",
    "            \n",
    "            # Finger landmark indices per MediaPipe hands documentation\n",
    "            finger_tips = [4, 8, 12, 16, 20]  # Thumb tip, Index tip, etc.\n",
    "            finger_bases = [2, 5, 9, 13, 17]  # Thumb IP, Index MCP, etc.\n",
    "            \n",
    "            finger_names = ['Thumb', 'Index', 'Middle', 'Ring', 'Pinky']\n",
    "            \n",
    "            for i, (tip_idx, base_idx) in enumerate(zip(finger_tips, finger_bases)):\n",
    "                extension = get_finger_extension(landmarks_px, tip_idx, base_idx)\n",
    "                percent = int(extension * 100)\n",
    "                \n",
    "                # Draw name and percent\n",
    "                y = y_start + i * gap\n",
    "                cv2.putText(frame, f'{finger_names[i]}: {percent}%', (info_x, y + 15),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # Draw bar graph background\n",
    "                cv2.rectangle(frame, (info_x, y + 30), (info_x + bar_width, y + 30 + bar_height),\n",
    "                              (50, 50, 50), -1)\n",
    "                \n",
    "                # Draw bar graph fill\n",
    "                cv2.rectangle(frame, (info_x, y + 30), (info_x + int(extension * bar_width), y + 30 + bar_height),\n",
    "                              (0, 191, 255), -1)\n",
    "            \n",
    "            # Palm size as distance wrist(0) to middle_finger_mcp(9)\n",
    "            palm_size = euclidean_dist(landmarks_px[0], landmarks_px[9])\n",
    "            palm_percent = int(min(palm_size / (w/4), 1) * 100)\n",
    "            \n",
    "            y = y_start + 5 * gap\n",
    "            cv2.putText(frame, f'Palm size (relative): {palm_percent}%', (info_x, y + 15),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            cv2.rectangle(frame, (info_x, y + 30), (info_x + bar_width, y + 30 + bar_height),\n",
    "                          (50, 50, 50), -1)\n",
    "            cv2.rectangle(frame, (info_x, y + 30), (info_x + int(palm_percent / 100 * bar_width), y + 30 + bar_height),\n",
    "                          (0, 255, 0), -1)\n",
    "    \n",
    "    cv2.imshow('MediaPipe Hand Tracking & Finger Measurement', frame)\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e15b0-7edd-4938-a22f-e4f4e664bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finger Joint Angles:\n",
    "\n",
    "It calculates the angle at the MCP (knuckle), PIP (middle joint), and DIP (near fingertip joint) of the index finger.\n",
    "\n",
    "The angles are computed based on the three-dimensional positions of the relevant joints, showing the degree the finger is bent at each joint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7064d9cf-0a21-4b57-930d-f5c7c2c57dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Create figure for plotting\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.invert_yaxis()  # Invert y-axis to match image coordinates\n",
    "ax.set_title('Face Landmarks')\n",
    "ax.set_aspect('equal')\n",
    "scatter = ax.scatter([], [], s=10, color='blue')\n",
    "\n",
    "# Function to update the plot with new landmarks\n",
    "def update_plot(face_points_normalized):\n",
    "    if face_points_normalized:\n",
    "        x = [p[0] for p in face_points_normalized]\n",
    "        y = [p[1] for p in face_points_normalized]\n",
    "        scatter.set_offsets(np.column_stack([x, y]))\n",
    "    return scatter,\n",
    "\n",
    "# Function to convert matplotlib figure to OpenCV image\n",
    "def fig_to_image(fig):\n",
    "    canvas = FigureCanvasAgg(fig)\n",
    "    canvas.draw()\n",
    "    buf = canvas.buffer_rgba()\n",
    "    w, h = canvas.get_width_height()\n",
    "    img_array = np.frombuffer(buf, dtype=np.uint8).reshape(h, w, 4)\n",
    "    img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)\n",
    "    return img_array\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "\n",
    "        # Clear the plot for new data\n",
    "        ax.clear()\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title('Face Landmarks')\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Draw landmarks on the video frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=frame,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1))\n",
    "\n",
    "                # Extract normalized landmark coordinates (0-1 range)\n",
    "                face_points_normalized = [(lm.x, lm.y) for lm in face_landmarks.landmark]\n",
    "                \n",
    "                # Plot landmarks\n",
    "                x = [p[0] for p in face_points_normalized]\n",
    "                y = [p[1] for p in face_points_normalized]\n",
    "                ax.scatter(x, y, s=10, color='blue')\n",
    "\n",
    "        # Convert the matplotlib figure to an OpenCV image\n",
    "        plot_img = fig_to_image(fig)\n",
    "        \n",
    "        # Resize plot image to match frame size for display\n",
    "        h, w = frame.shape[:2]\n",
    "        plot_img = cv2.resize(plot_img, (w, h))\n",
    "        \n",
    "        # Display both the video feed and the plot side by side\n",
    "        combined_img = np.hstack((frame, plot_img))\n",
    "        \n",
    "        # If the combined image is too large, resize it\n",
    "        if combined_img.shape[1] > 1920:  # Standard HD width\n",
    "            scale_factor = 1920 / combined_img.shape[1]\n",
    "            combined_img = cv2.resize(combined_img, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "        \n",
    "        cv2.imshow('Face Mesh with Real-time Plot', combined_img)\n",
    "        \n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):  # Press ESC to exit\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a197c-7fb8-4075-8774-05392beb0e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10(.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
